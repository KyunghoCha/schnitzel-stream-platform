# schnitzel-stream-platform

![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python)
![Status](https://img.shields.io/badge/Status-Beta-yellow)
![License](https://img.shields.io/badge/License-Apache%202.0-blue)

> Extensible real-time video event pipeline
> í™•ì¥ ê°€ëŠ¥í•œ ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì´ë²¤íŠ¸ íŒŒì´í”„ë¼ì¸

Python baseline: **3.11** (í•˜ìœ„/ìƒìœ„ ë²„ì „ì€ ì˜ì¡´ì„± ê²€ì¦ í›„ ë‹¨ê³„ì ìœ¼ë¡œ í™•ì¥)

---

## ğŸ¯ Overview | ê°œìš”

### English

This project provides a production-ready AI pipeline that:

- Connects to CCTV cameras via **RTSP** (with auto-reconnect)
- Runs **object detection** models (YOLO, ONNX, or custom)
- Detects domain events (intrusion, PPE violation, posture, hazard)
- Sends structured events to a **backend API**

Designed for **plug-and-play** extensibility: add new models or event types via config, not code.

### í•œêµ­ì–´

ì´ í”„ë¡œì íŠ¸ëŠ” ì‹¤ì‹œê°„ ì˜ìƒ ì´ë²¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ AI íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤:

- **RTSP** ì¹´ë©”ë¼ ì—°ê²° (ìë™ ì¬ì—°ê²° ì§€ì›)
- **ê°ì²´ íƒì§€** ëª¨ë¸ ì‹¤í–‰ (YOLO, ONNX, ì»¤ìŠ¤í…€)
- ì•ˆì „ ì´ë²¤íŠ¸ ê°ì§€ (ì¹¨ì…, ë³´í˜¸êµ¬ ë¯¸ì°©ìš©, ìì„¸ ì´ìƒ, ìœ„í—˜ ìƒí™©)
- **ë°±ì—”ë“œ API**ë¡œ êµ¬ì¡°í™”ëœ ì´ë²¤íŠ¸ ì „ì†¡

ì„¤ì •ë§Œìœ¼ë¡œ ìƒˆ ëª¨ë¸/ì´ë²¤íŠ¸ íƒ€ì…ì„ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” **í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°**ì…ë‹ˆë‹¤.

---

## ğŸ—ï¸ Architecture | ì•„í‚¤í…ì²˜

```mermaid
flowchart LR
    subgraph Input ["ğŸ“¥ Input"]
        direction LR
        subgraph IN_VIDEO ["ğŸ¥ Video Inputs"]
            direction TB
            A["ğŸ“¹ RTSP Camera"]
            B["ğŸ¬ Video File"]
            K["ğŸ“· Webcam"]
            L["ğŸ§© Source Plugin"]
        end
        subgraph IN_SENSOR ["ğŸ“¡ Sensor Inputs"]
            direction TB
            S1A["Sensor A (optional)"]
            S1B["Sensor B (optional)"]
            S1N["Sensor N (optional)"]
        end
    end

    subgraph PIPE ["âš™ï¸ AI Pipeline"]
        LC{"is_live?"}
        T["ThreadedSource (live only)"]
        C["Frame Sampler"]
        subgraph Models ["ğŸ§  Model Adapter"]
            D1["YOLO"]
            D2["ONNX"]
            D3["Custom"]
        end
        E["Tracker (optional)"]
        F["Zone Rules + Dedup (optional)"]
        S2["Sensor Runtime (single/multi)"]
        G["Event Builder + Sensor Attach"]
        X["Event Mux"]
    end

    subgraph Output ["ğŸ“¤ Output"]
        O1["ğŸŒ Backend"]
        O2["ğŸ“ JSONL"]
        O3["ğŸ–¨ï¸ Stdout"]
        O4["ğŸ§© Custom Emitter"]
    end

    VIN{"Select"}
    VOUT{"Select"}

    A --> VIN
    B --> VIN
    K --> VIN
    L --> VIN
    VIN --> LC
    LC -- true --> T --> C
    LC -- false --> C
    S1A --> S2
    S1B --> S2
    S1N --> S2

    C --> D1
    C --> D2
    C --> D3
    D1 --> E
    D2 --> E
    D3 --> E
    E --> F
    S2 --> G
    F --> G
    S2 -.-> X
    G --> X --> VOUT
    VOUT --> O1
    VOUT --> O2
    VOUT --> O3
    VOUT --> O4
```

> **Model Extensibility**: `AI_MODEL_ADAPTER` í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë¸ êµì²´/ì¶”ê°€. ì½¤ë§ˆ êµ¬ë¶„ìœ¼ë¡œ ë©€í‹° ëª¨ë¸ ë™ì‹œ ì‹¤í–‰(ì˜ˆ: `adapterA:ClassA,adapterB:ClassB`).
>
> | Adapter | ì„¤ëª… |
> |:---|:---|
> | `YOLOAdapter` | Ultralytics YOLO (pt) |
> | `ONNXYOLOAdapter` | ONNX ìµœì í™” ì¶”ë¡  |
> | `Custom` | `infer(frame)` í•˜ë‚˜ë§Œ êµ¬í˜„ |

> **Input/Output Contract**: ë‹¤ì´ì–´ê·¸ë¨ì˜ `Select`ëŠ” ì…ë ¥/ì¶œë ¥ ëª¨ë‘ `one-of` ë‹¨ì¼ ê²½ë¡œ ì„ íƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. (fan-in/fan-out ì•„ë‹˜)
>
> **Sensor Extensibility**: `AI_SENSOR_ENABLED=true` + `AI_SENSOR_ADAPTER`(ë‹¨ì¼) ë˜ëŠ” `AI_SENSOR_ADAPTERS`(ì½¤ë§ˆ êµ¬ë¶„ ë‹¤ì¤‘)ë¡œ ì„¼ì„œ í”ŒëŸ¬ê·¸ì¸ì„ ë¡œë“œí•´ ì´ë²¤íŠ¸ `sensor` í•„ë“œì— ì£¼ì….
>
> **Output Selection**: ëŸ°íƒ€ì„ì€ fan-outì´ ì•„ë‹ˆë¼ ë‹¨ì¼ ê²½ë¡œ ì„ íƒì…ë‹ˆë‹¤ (`--output-jsonl` > `--dry-run` > `AI_EVENTS_EMITTER_ADAPTER` > backend).
>
> **Final Decision Ownership**: ê¸°ë³¸ê°’ì€ ëŸ°íƒ€ì„ì´ ê°ì§€/ìœµí•© ê·¼ê±° ì´ë²¤íŠ¸ë¥¼ ì „ë‹¬í•˜ê³ , ìµœì¢… ìœ„í—˜/ì•ŒëŒ íŒë‹¨ì€ ì‚¬ìš©ì ìš´ì˜ ì •ì±…ì—ì„œ ê²°ì •í•©ë‹ˆë‹¤. í•„ìš” ì‹œ ë°±ì—”ë“œ ë£° ì—”ì§„ ë˜ëŠ” ëŸ°íƒ€ì„ ë‚´ë¶€ ë£°/í”ŒëŸ¬ê·¸ì¸ìœ¼ë¡œ ìµœì¢… íŒë‹¨ ë‹¨ê³„ë¥¼ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

## âš¡ Quickstart | ë¹ ë¥¸ ì‹œì‘

### 1) Clone & Install

```bash
git clone https://github.com/KyunghoCha/schnitzel-stream-platform.git
cd schnitzel-stream-platform
pip install -r requirements.txt
```

### 2) Dry-Run (Sample Video, No Backend)

Windows (Recommended):

```powershell
$env:AI_MODEL_MODE="mock"
$env:AI_ZONES_SOURCE="none"
./setup_env.ps1
python -m ai.pipeline `
  --dry-run `
  --max-events 20
```

Linux/Bash:

```bash
export AI_MODEL_MODE=mock
export AI_ZONES_SOURCE=none
export PYTHONPATH=src

python -m ai.pipeline \
  --dry-run \
  --max-events 20
```

### 3) Webcam Dry-Run (No Backend)

Windows:

```powershell
$env:AI_MODEL_MODE="mock"
$env:AI_ZONES_SOURCE="none"
./setup_env.ps1
python -m ai.pipeline `
  --source-type webcam `
  --camera-index 0 `
  --dry-run `
  --max-events 20
```

Linux/Bash:

```bash
export AI_MODEL_MODE=mock
export AI_ZONES_SOURCE=none
export PYTHONPATH=src

python -m ai.pipeline \
  --source-type webcam \
  --camera-index 0 \
  --dry-run \
  --max-events 20
```

### 4) RTSP Quick Test (Explicit Mock)

Windows:

```powershell
$env:AI_MODEL_MODE="mock"
$env:AI_ZONES_SOURCE="none"
./setup_env.ps1
python -m ai.pipeline `
  --source-type rtsp `
  --camera-id cam01 `
  --dry-run `
  --max-events 20
```

Linux/Bash:

```bash
export AI_MODEL_MODE=mock
export AI_ZONES_SOURCE=none
export PYTHONPATH=src

python -m ai.pipeline \
  --source-type rtsp \
  --camera-id cam01 \
  --dry-run \
  --max-events 20
```

### 5) Real YOLO Minimum Example

Real model runtime requires model dependencies:

Windows:

```powershell
pip install -r requirements-model.txt
$env:AI_MODEL_MODE="real"
$env:AI_MODEL_ADAPTER="ai.vision.adapters.yolo_adapter:YOLOAdapter"
$env:YOLO_MODEL_PATH="models/model.pt"
./setup_env.ps1
python -m ai.pipeline `
  --source-type rtsp `
  --camera-id cam01
```

Linux/Bash:

```bash
pip install -r requirements-model.txt

export AI_MODEL_MODE=real
export AI_MODEL_ADAPTER=ai.vision.adapters.yolo_adapter:YOLOAdapter
export YOLO_MODEL_PATH=models/model.pt
export PYTHONPATH=src

python -m ai.pipeline \
  --source-type rtsp \
  --camera-id cam01
```

That's it! ì´ë²¤íŠ¸ ë¡œê·¸ê°€ í„°ë¯¸ë„ì— ì¶œë ¥ë©ë‹ˆë‹¤.
Real model pathì—ì„œëŠ” `requirements-model.txt` ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

---

## ğŸ“¦ Installation | ì„¤ì¹˜

### Core (Required)

```bash
pip install -r requirements.txt
```

Python: **3.10+** (runtime uses modern typing syntax such as `A | B`).

### Windows Environment Setup (Optional)

```powershell
# Auto-set PYTHONPATH and verify 'src' directory
./setup_env.ps1
```

### Development (Tests, Dashboard)

```bash
pip install -r requirements-dev.txt
```

### Model Runtime (YOLO/ONNX)

```bash
pip install -r requirements-model.txt
```

> **GPU (CUDA) on Windows**: PyTorch ì¬ì„¤ì¹˜ í•„ìš”
>
> ```bash
> pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124
> ```

---

## ğŸ”§ Configuration | ì„¤ì •

### Runtime (Production-Focused)

| í™˜ê²½ë³€ìˆ˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
| :--- | :--- | :--- |
| `AI_SOURCE_TYPE` | ì…ë ¥ ì†ŒìŠ¤ íƒ€ì… (`file`/`rtsp`/`webcam`/`plugin`) | `file` |
| `AI_SOURCE_ADAPTER` | ì…ë ¥ Source í”ŒëŸ¬ê·¸ì¸ ê²½ë¡œ (`module:ClassName`, `AI_SOURCE_TYPE=plugin`ì¼ ë•Œ) | - |
| `AI_SENSOR_ENABLED` | ì„¼ì„œ ì¶• ìŠ¤ìœ„ì¹˜ (`true`ë©´ ì„¼ì„œ íŒ¨í‚·ì„ ì´ë²¤íŠ¸ì— ì£¼ì…) | `false` |
| `AI_SENSOR_TYPE` | ì„¼ì„œ íƒ€ì… íŒíŠ¸ (`ros2`/`mqtt`/`modbus`/`serial`/`plugin`/custom ì˜ˆ: `ultrasonic`) | - |
| `AI_SENSOR_ADAPTER` | ì„¼ì„œ í”ŒëŸ¬ê·¸ì¸ ë‹¨ì¼ ê²½ë¡œ (`module:ClassName`) | - |
| `AI_SENSOR_ADAPTERS` | ì„¼ì„œ í”ŒëŸ¬ê·¸ì¸ ë‹¤ì¤‘ ê²½ë¡œ (ì½¤ë§ˆ êµ¬ë¶„, ì˜ˆ: `a:Front,b:Rear`) | - |
| `AI_SENSOR_TOPIC` | ì„¼ì„œ í† í”½/ì±„ë„ íŒíŠ¸ | - |
| `AI_SENSOR_QUEUE_SIZE` | ì„¼ì„œ ëŸ°íƒ€ì„ ë²„í¼ í¬ê¸° | `256` |
| `AI_SENSOR_TIME_WINDOW_MS` | ì´ë²¤íŠ¸-ì„¼ì„œ ë§¤ì¹­ ì‹œê°„ì°½(ms) | `300` |
| `AI_SENSOR_EMIT_EVENTS` | ë…ë¦½ `SENSOR_EVENT` ì „ì†¡ í™œì„±í™” | `false` |
| `AI_SENSOR_EMIT_FUSED_EVENTS` | ì¶”ê°€ `FUSED_EVENT` ì „ì†¡ í™œì„±í™” | `false` |
| `AI_MODEL_MODE` | `real` / `mock` (`ìš´ì˜ ê¶Œì¥: real`) | `real` |
| `AI_MODEL_ADAPTER` | Adapter ëª¨ë“ˆ ê²½ë¡œ (real ëª¨ë“œ í•„ìˆ˜, ê¸°ë³¸ í…œí”Œë¦¿ì€ êµ¬í˜„ ì „ fail-fast) | `ai.vision.adapters.custom_adapter:CustomModelAdapter` |
| `AI_EVENTS_EMITTER_ADAPTER` | ì¶œë ¥ Emitter í”ŒëŸ¬ê·¸ì¸ ê²½ë¡œ (`module:ClassName`) | - |
| `AI_RTSP_TRANSPORT` | RTSP ì „ì†¡ í”„ë¡œí† ì½œ (`tcp`/`udp`) | `tcp` |
| `AI_ZONES_SOURCE` | Zone ì†ŒìŠ¤ (`api`/`file`/`none`, `none`ì´ë©´ zone í‰ê°€ ë¹„í™œì„±) | `api` |
| `AI_LOG_LEVEL` | ë¡œê·¸ ë ˆë²¨ (`DEBUG`/`INFO`/`WARNING`) | `INFO` |

### Integration / Plugin Optional

| í™˜ê²½ë³€ìˆ˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
| :--- | :--- | :--- |
| `AI_ROS2_SOURCE_TOPIC` | ROS2 ì…ë ¥ í† í”½ (ROS2 source plugin ì‚¬ìš© ì‹œ) | `/camera/image_raw/compressed` |
| `AI_ROS2_EVENT_TOPIC` | ROS2 ì¶œë ¥ í† í”½ (ROS2 emitter plugin ì‚¬ìš© ì‹œ) | `/ai/events` |
| `YOLO_MODEL_PATH` | YOLO ê°€ì¤‘ì¹˜ ê²½ë¡œ | - |
| `ONNX_MODEL_PATH` | ONNX ëª¨ë¸ ê²½ë¡œ | - |
| `TRACKER_TYPE` | `none` / `iou` / `bytetrack` | `none` |
| `MODEL_CLASS_MAP_PATH` | í´ë˜ìŠ¤ ë§¤í•‘ YAML | - |

### Test / Mock / Fake Only

| í™˜ê²½ë³€ìˆ˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
| :--- | :--- | :--- |
| `AI_MODEL_MODE=mock` | í…ŒìŠ¤íŠ¸/ê²€ì¦ìš© ëª¨ë“œ (ìš´ì˜ ë¹„ê¶Œì¥) | - |
| `AI_FAKE_SENSOR_ID` | fake ì„¼ì„œ ID(ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©) | `ultrasonic-front-01` |
| `AI_FAKE_SENSOR_INTERVAL_SEC` | fake ì„¼ì„œ íŒ¨í‚· ê°„ê²©(ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©) | `0.05` |
| `LOG_DIR` | RTSP E2E/ë¡œì»¬ ê²€ì‚¬ìš© ë¡œê·¸ ë””ë ‰í„°ë¦¬ | - |
| `CHECK_PORT` | RTSP E2E ìŠ¤í¬ë¦½íŠ¸ í¬íŠ¸ ì²´í¬ ì‹œì‘ì  | - |

ì „ì²´ ì„¤ì •: `configs/default.yaml`

ì°¸ê³ : `PYTHON_BIN`ì€ `scripts/legacy/*.sh`ì—ì„œë§Œ ì‚¬ìš©í•˜ëŠ” ë ˆê±°ì‹œ ë³€ìˆ˜ì…ë‹ˆë‹¤. í˜„ì¬ Python ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸(`scripts/*.py`)ëŠ” `sys.executable`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì°¸ê³ : `AI_SENSOR_ENABLED=true`ì´ë©´ ì„¼ì„œ í”ŒëŸ¬ê·¸ì¸ì—ì„œ ì½ì€ ìµœê·¼ íŒ¨í‚·ì„ ê° ë¹„ì „ ì´ë²¤íŠ¸ì˜ `sensor` í•„ë“œì— ì£¼ì…í•©ë‹ˆë‹¤. `AI_SENSOR_ADAPTERS`ë¥¼ ì“°ë©´ ì—¬ëŸ¬ ì„¼ì„œë¥¼ ë™ì‹œì— ìˆ˜ì§‘í•˜ê³  ì‹œê°„ì°½ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ì„¼ì„œ íŒ¨í‚·ì„ ë¶™ì…ë‹ˆë‹¤. `AI_SENSOR_EMIT_EVENTS=true`ë©´ ë…ë¦½ `SENSOR_EVENT`, `AI_SENSOR_EMIT_FUSED_EVENTS=true`ë©´ ì¶”ê°€ `FUSED_EVENT`ë„ ì „ì†¡í•©ë‹ˆë‹¤. ìµœì¢… ìœ„í—˜/ì•ŒëŒ íŒë‹¨ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ì ìš´ì˜ ì •ì±…ì—ì„œ ê²°ì •í•˜ê³ , í•„ìš” ì‹œ ë°±ì—”ë“œ/ëŸ°íƒ€ì„ í™•ì¥ìœ¼ë¡œ ìë™í™”í•©ë‹ˆë‹¤.

### ROS2 Plugin Quickstart (Optional)

```bash
# ROS2 source plugin (input)
export AI_SOURCE_TYPE=plugin
export AI_SOURCE_ADAPTER=ai.plugins.ros2.image_source:Ros2ImageSource
export AI_ROS2_SOURCE_TOPIC=/camera/image_raw/compressed

# ROS2 emitter plugin (output)
export AI_EVENTS_EMITTER_ADAPTER=ai.plugins.ros2.event_emitter:Ros2EventEmitter
export AI_ROS2_EVENT_TOPIC=/ai/events

# Run
AI_MODEL_MODE=mock \
PYTHONPATH=src \
python -m ai.pipeline \
  --max-events 20
```

ROS2 plugin modules require ROS2 Python packages (`rclpy`, `sensor_msgs`, `std_msgs`).

### Sensor Lane Quickstart (No Hardware)

```bash
export AI_MODEL_MODE=mock
export AI_SENSOR_ENABLED=true
export AI_SENSOR_TYPE=ultrasonic
export AI_SENSOR_ADAPTER=ai.plugins.sensors.fake_ultrasonic:FakeUltrasonicSensorSource
export AI_SENSOR_TIME_WINDOW_MS=5000
export AI_SENSOR_EMIT_EVENTS=true
export AI_SENSOR_EMIT_FUSED_EVENTS=true

PYTHONPATH=src \
python -m ai.pipeline \
  --output-jsonl outputs/events_sensor.jsonl \
  --max-events 8
```

### Multi-Sensor Adapter Example

```bash
export AI_MODEL_MODE=mock
export AI_SENSOR_ENABLED=true
export AI_SENSOR_ADAPTERS="ai.plugins.sensors.fake_ultrasonic:FakeUltrasonicSensorSource,\
ai.plugins.sensors.serial_ultrasonic:SerialUltrasonicSensorSource"
export AI_SENSOR_TIME_WINDOW_MS=5000

PYTHONPATH=src \
python -m ai.pipeline \
  --dry-run \
  --max-events 20
```

---

## ğŸ“š Documentation | ë¬¸ì„œ

| ë¬¸ì„œ | ì„¤ëª… |
| :--- | :--- |
| [docs/index.md](docs/index.md) | ë¬¸ì„œ ì¸ë±ìŠ¤ |
| [docs/progress/roadmap.md](docs/progress/roadmap.md) | ì§„í–‰ ë¡œë“œë§µ |
| [docs/specs/pipeline_spec.md](docs/specs/pipeline_spec.md) | íŒŒì´í”„ë¼ì¸ ëª…ì„¸ |
| [docs/specs/model_interface.md](docs/specs/model_interface.md) | ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ê³„ì•½ |
| [docs/ops/ops_runbook.md](docs/ops/ops_runbook.md) | ìš´ì˜ ê°€ì´ë“œ |
| [docs/future/future_roadmap.md](docs/future/future_roadmap.md) | ë¡œë“œë§µ (ë‹¤ì¤‘ ì¹´ë©”ë¼, ììœ¨í•™ìŠµ) |

### Contracts & Policy | ê³„ì•½ ë° ìš´ì˜ ì •ì±…

íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬ ê³„ì•½(ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ/ëª¨ë¸ ì…ì¶œë ¥/ë¶„ë¥˜ ì •ì±…)ì„ ê´€ë¦¬í•˜ëŠ” ë¬¸ì„œì…ë‹ˆë‹¤.

| ë¬¸ì„œ | ì„¤ëª… |
| :--- | :--- |
| [docs/contracts/protocol.md](docs/contracts/protocol.md) | ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ë° ì „ì†¡ ê³„ì•½ |
| [docs/specs/model_interface.md](docs/specs/model_interface.md) | ëª¨ë¸ ì–´ëŒ‘í„° ì…ì¶œë ¥ ê³„ì•½ |
| [docs/specs/model_class_taxonomy.md](docs/specs/model_class_taxonomy.md) | í´ë˜ìŠ¤ ë¶„ë¥˜/ì •ì±… ê¸°ì¤€ |
| [docs/progress/roadmap.md](docs/progress/roadmap.md) | ê²°ì • í•„ìš” í•­ëª© ë° ì§„í–‰ í˜„í™© |

---

## ğŸ§ª Testing | í…ŒìŠ¤íŠ¸

```bash
# Unit + Integration
PYTEST_DISABLE_PLUGIN_AUTOLOAD=1 pytest -q

# Test hygiene (duplicate/meaningless test scan)
python scripts/test_hygiene.py

# RTSP E2E (sample video)
# (script forces AI_MODEL_MODE=mock explicitly)
python scripts/check_rtsp.py

# Regression (mock explicit)
AI_MODEL_MODE=mock PYTHONPATH=src python scripts/regression_check.py

# Multi-camera
python scripts/multi_cam.py start
python scripts/multi_cam.py stop
```

---

## ğŸ³ Docker

```bash
# Build
docker build -t schnitzel-stream-platform .

# Run (dry-run test path, explicit mock)
docker run --rm -e AI_MODEL_MODE=mock -v /tmp/snapshots:/data/snapshots schnitzel-stream-platform

# Run (real mode, requires concrete adapter/model config)
docker run --rm \
  -e AI_MODEL_MODE=real \
  -e AI_MODEL_ADAPTER=ai.vision.adapters.yolo_adapter:YOLOAdapter \
  -e YOLO_MODEL_PATH=/models/model.pt \
  schnitzel-stream-platform python -m ai.pipeline
```

---

## ğŸ“Š Project Status | í˜„í™©

| í•­ëª© | ìƒíƒœ |
| :--- | :--- |
| Non-model Pipeline | âœ… Complete |
| RTSP E2E (host/docker) | âœ… Complete |
| Model Adapters (YOLO/ONNX) | âœ… Complete |
| Multi-model Merge | âœ… Complete |
| Class Mapping | âœ… Complete |
| IOU Tracker | âœ… Complete |
| Codebase Audit (169 items) | âœ… Complete |
| Document Consistency (150+ files) | âœ… Complete |
| Tests (153 passed, 2 skipped on 2026-02-11) | âœ… Complete |
| Cross-Platform CI Matrix (Linux/Windows/macOS) | ğŸŸ¨ Configured (run on GitHub Actions) |
| Real RTSP Device | ğŸ”² Pending (ì‹¤í™˜ê²½ ì—°ë™ ëŒ€ê¸°) |
| Real Backend Integration | ğŸ”² Pending (ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ë™ ëŒ€ê¸°) |
| Production Model Accuracy | ğŸ”² Pending (ëª¨ë¸ í™•ì • í›„) |
| IoT Sensor Integration | ğŸŸ¨ Baseline Complete (sensor/fused runtime, hardware integration pending) |

---

## ğŸ“ License | ë¼ì´ì„¼ìŠ¤

Apache License 2.0 (`LICENSE`)

---

## ğŸ¤ Contributing | ê¸°ì—¬

1. Fork this repository
2. Create a feature branch
3. Submit a Pull Request

---

<p align="center">
  Made by <b>Kyungho Cha</b>
  <br>
  Copyright (c) 2026. All rights reserved.
</p>
